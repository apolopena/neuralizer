# ==============================================================================
# NeurALIzer - Environment Configuration
# ==============================================================================
#
# Copy to .env and fill in values before starting the stack:
#   cp .env.example .env
#
# ==============================================================================

# ==============================================================================
# BACKEND
# ==============================================================================
BACKEND_HOST_PORT=8000
REDIS_HOST=redis
REDIS_PORT=6379

# ==============================================================================
# LLM INFERENCE
# ==============================================================================
#
# GPU backend image variants:
#   server-cuda     NVIDIA (default, best performance)
#   server-vulkan   AMD / Intel / NVIDIA (cross-vendor)
#   server          CPU-only (no GPU required, much slower)
#
LLM_IMAGE=ghcr.io/ggml-org/llama.cpp:server-cuda
#
# Context window size in tokens.
# Qwen3-4B supports up to 32768. Reduce if hitting OOM on your GPU.
#
LLM_CONTEXT_SIZE=32768
