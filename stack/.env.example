# ==============================================================================
# NeurALIzer - Environment Configuration
# ==============================================================================
#
# Copy to .env and fill in values before starting the stack:
#   cp .env.example .env
#
# ==============================================================================

# ==============================================================================
# BACKEND
# ==============================================================================
BACKEND_HOST_PORT=8000
REDIS_HOST=redis
REDIS_PORT=6379

# ==============================================================================
# LLM INFERENCE
# ==============================================================================
#
# GPU backend image variants:
#   server-cuda     NVIDIA (default, best performance)
#   server-vulkan   AMD / Intel / NVIDIA (cross-vendor)
#   server          CPU-only (no GPU required, much slower)
#
LLM_IMAGE=ghcr.io/ggml-org/llama.cpp:server-cuda
#
# Model filename in stack/models/. Switch between:
#   qwen3-4b-instruct-2507-q8_0.gguf    (fast, non-thinking â€” default)
#   qwen3-4b-thinking-2507-q8_0.gguf    (slow, deep reasoning)
#
LLM_MODEL=qwen3-4b-instruct-2507-q8_0.gguf
#
# Context window size in tokens.
# Qwen3-4B supports up to 32768. Reduce if hitting OOM on your GPU.
#
LLM_CONTEXT_SIZE=32768
#
# Timeout in seconds for LLM API calls. Increase for thinking models.
#
LLM_TIMEOUT=15

# ==============================================================================
# OPEN WEBUI
# ==============================================================================
OPENWEBUI_HOST_PORT=8081
