# ==============================================================================
# NeurALIzer - Environment Configuration
# ==============================================================================
#
# Copy to .env and fill in values before starting the stack:
#   cp .env.sample .env
#
# ==============================================================================

# ==============================================================================
# COMPOSE FILES
# ==============================================================================
#
# Uses docker compose's native COMPOSE_FILE variable to layer override files.
# Mix and match: pick a GPU backend, optionally add HTTPS.
# Just change the line and run: docker compose up -d
#
# GPU backends:
#   docker-compose.cuda.yml      NVIDIA CUDA
#   docker-compose.vulkan.yml    AMD / Intel / NVIDIA via Vulkan
#   docker-compose.cpu.yml       CPU only (no GPU)
#
# Optional:
#   docker-compose.https.yml     Adds HTTPS port (also set CADDYFILE below)
#   docker-compose.dev.yml       Exposes backend + Open WebUI ports to host
#
#
COMPOSE_FILE=docker-compose.yml:docker-compose.cuda.yml
#
# Other examples:
# COMPOSE_FILE=docker-compose.yml:docker-compose.vulkan.yml
# COMPOSE_FILE=docker-compose.yml:docker-compose.cpu.yml
# COMPOSE_FILE=docker-compose.yml:docker-compose.cuda.yml:docker-compose.https.yml
# COMPOSE_FILE=docker-compose.yml:docker-compose.vulkan.yml:docker-compose.https.yml
# COMPOSE_FILE=docker-compose.yml:docker-compose.cuda.yml:docker-compose.dev.yml

# ==============================================================================
# CADDYFILE
# ==============================================================================
#
# Which Caddyfile to mount. Default serves plain HTTP.
#
# For HTTPS, you need BOTH:
#   1. Add docker-compose.https.yml to COMPOSE_FILE above (opens port 443)
#   2. Uncomment CADDYFILE below (tells Caddy to serve HTTPS)
#
# CADDYFILE=Caddyfile.https

# ==============================================================================
# PORTS
# ==============================================================================
HTTP_PORT=80
OPENWEBUI_PORT=8082
# HTTPS_PORT=443              # Only with HTTPS. Change if 443 is in use.

# ==============================================================================
# LLM INFERENCE
# ==============================================================================
#
# Leave LLM_IMAGE blank — it is auto-set by the GPU config above.
# Only set this if you need a custom or pinned image. Advanced users only.
# LLM_IMAGE=
#
# Model filename in stack/models/. Switch between:
#   qwen3-4b-instruct-2507-q8_0.gguf    (fast, non-thinking — default)
#   qwen3-4b-thinking-2507-q8_0.gguf    (slow, deep reasoning)
#
LLM_MODEL=qwen3-4b-instruct-2507-q8_0.gguf
#
# Context window size in tokens.
# Qwen3-4B supports up to 32768. Reduce if hitting OOM on your GPU.
#
LLM_CONTEXT_SIZE=32768
#
# Timeout in seconds for LLM API calls. Increase for thinking models.
#
LLM_TIMEOUT=120

# ==============================================================================
# BACKEND
# ==============================================================================
REDIS_HOST=redis
REDIS_PORT=6379

# ==============================================================================
# DEV PORTS (bypasses Caddy)
# ==============================================================================
#
# These expose services directly on the host, bypassing Caddy.
# Used when docker-compose.dev.yml is added to COMPOSE_FILE above.
#
# BACKEND_HOST_PORT=8000
# OPENWEBUI_HOST_PORT=8081
